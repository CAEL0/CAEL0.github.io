---
title: "밑바닥부터 시작하는 딥러닝 Chapter 7"
excerpt: 합성곱 신경망 (CNN)
categories: [Deep Learning]
tags: [밑바닥부터 시작하는 딥러닝]
last_modified_at: 2021-08-16 18:33:00 +0900
---

> *이 포스트는 책 [<u>밑바닥부터 시작하는 딥러닝</u>](https://books.google.co.kr/books/about/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D.html?id=SM9KDwAAQBAJ&source=kp_book_description&redir_esc=y)을 정리한 내용입니다.*


<br>

**합성곱 신경망** (Convolutional Neural Network) : 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는 인공 신경망

**합성곱** (convolution) : 두 함수 중 하나를 반전, 이동시켜가며 나머지 함수와의 곱을 연이어 적분

$$ (f * g)(t) = \int_{-\infty}^\infty f(\tau)g(t - \tau)d\tau $$

> # 7.1 전체 구조
---

지금까지 본 신경망처럼 계층을 조합하여 만들 수 있음

**합성곱 계층** (convolutional layer)과 **풀링 계층** (pooling layer)이 새롭게 등장

**완전연결** (fully-connected) : 인접하는 계층의 모든 뉴런과 결합된 형태의 신경망, 지금까지 Affine 계층이라고 부름

<br>

![완전연결 네트워크](/assets/images/2021_08_16/7_1_1.PNG)

<br>

Affine 계층과 ReLU 계층으로 이뤄진 네트워크

<br>

![CNN 네트워크](/assets/images/2021_08_16/7_1_2.PNG)

<br>

합성곱 계층 (Conv)과 풀링 계층 (Pooling)이 추가됨

3번째 층처럼 풀링 계층은 생략하기도 함

Affine-ReLU $ \rightarrow $ Conv-ReLU-(Pooling)

출력에 가까운 층에서는 Affine-ReLU 구성을 사용할 수 있음

마지막 출력 층은 Affine-Softmax 조합을 그대로 사용

<br>

> # 7.2 합성곱 계층
---

>> ## 7.2.1 완전연결 계층의 문제점
---

**데이터의 형상이 무시됨**

데이터가 이미지일 경우, 보통 이미지는 가로, 세로, 채널 (색상)으로 구성된 3차원 데이터이지만 완전연결 계층에 입력시킬 땐 1차원 데이터로 평탄화해줘야 함

MNIST 데이터셋을 예로 들면, 형상이 (1, 28, 28)이었던 이미지를 (784, )로 바꿔 입력시킴

이렇게 하면 데이터의 공간적 정보를 살릴 수 없음

$ \rightarrow $ CNN은 데이터의 형상을 유지함

**특징 맵** (feature map) : CNN에서 합성곱 계층의 입출력 데이터

<br>

>> ## 7.2.2 합성곱 연산
---

<br>

![합성곱 연산](/assets/images/2021_08_16/7_2_2_1.PNG)

<br>

데이터의 형상 = (높이, 너비) = (행 개수, 열 개수)

입력 : (4, 4) / 필터 (커널) : (3, 3) / 출력 : (2, 2)

**윈도우** (window) : 필터가 입력 데이터와 겹치는 부분

**단일 곱셈-누산** (fused multiply-add, FMA) : 대응하는 원소끼리 곱한 후 총합을 구하는 계산

<br>

![합성곱 연산 과정](/assets/images/2021_08_16/7_2_2_2.PNG)

<br>

윈도우를 일정 간격 이동해가며 FMA를 시행함

CNN에서의 필터의 매개변수 = 완전연결 신경망에서의 가중치 매개변수

<br>

![합성곱 연산 편향](/assets/images/2021_08_16/7_2_2_3.PNG)

<br>

FMA 결과의 각 원소에 편향을 더해주면 출력 데이터가 됨

<br>

>> ## 7.2.3 패딩
---

**패딩** (padding) : 입력 데이터 주변을 특정 값으로 채움

<br>

![패딩 예시](/assets/images/2021_08_16/7_2_3.PNG)

<br>

(4, 4) 크기의 입력 데이터에 폭이 1인 패딩을 적용

패딩이 추가되어 입력 데이터의 크기가 (6, 6)이 됐고, (4, 4) 크기의 출력 데이터가 생성됨

패딩은 주로 출력 크기를 조정할 목적으로 사용

<br>

>> ## 7.2.4 스트라이드
---

**스트라이드** (stride) : 필터를 적용하는 위치의 간격

<br>

![스트라이드 2 예시](/assets/images/2021_08_16/7_2_4.PNG)

<br>

스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동

입력 크기 : (H, W) / 필터 크기 : (FH, FW) / 출력 크기 : (OH, OW) / 패딩 : P / 스트라이드 : S

$$ OH = \frac{H + 2P - FH}{S} + 1 $$

$$ OW = \frac{W + 2P - FW}{S} + 1 $$

<br>

>> ## 7.2.5 3차원 데이터의 합성곱 연산
---

<br>

![3차원 합성곱 예시](/assets/images/2021_08_16/7_2_5_1.PNG)

<br>

![3차원 합성곱 과정](/assets/images/2021_08_16/7_2_5_2.PNG)

<br>

* 입력 데이터의 채널 수와 필터의 채널 수가 같아야함

* 모든 필터의 크기가 같아야함

<br>

>> ## 7.2.6 블록으로 생각하기
---

3차원의 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생각

입력 데이터 : 채널 수 C / 높이 H / 너비 W $ \rightarrow $ (C, H, W)

필터 : 채널 수 C / 높이 FH / 너비 FW $ \rightarrow $ (C, FH, FW)

<br>

![3차원 합성곱 블록](/assets/images/2021_08_16/7_2_6_1.PNG)

<br>

필터를 1개만 사용하면 출력 데이터의 채널은 1개임

<br>

![3차원 합성곱 다수의 필터](/assets/images/2021_08_16/7_2_6_2.PNG)

<br>

필터를 FN개 적용 $ \rightarrow $ 출력 맵의 채널도 FN개

그러므로 필터의 가중치 데이터는 4차원 : (출력 채널 수, 입력 채널 수, 높이, 너비)

<br>

![3차원 합성곱 다수의 필터 + 편향](/assets/images/2021_08_16/7_2_6_3.PNG)

<br>

편향은 채널 하나에 값 하나씩으로 구성됨 : (FN, 1, 1)

<br>

>> ## 7.2.7 배치 처리
---

각 계층에 흐르는 데이터의 차원을 하나 늘려 4차원으로 저장

(데이터 수, 채널 수, 높이, 너비)

<br>

![배치 처리](/assets/images/2021_08_16/7_2_7.PNG)

<br>

